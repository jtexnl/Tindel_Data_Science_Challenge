{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Scenario\n",
    "\n",
    "I am a part of a restaurant company based in the United States with branches across the country and in some parts of Canada and the UK. The company is about to launch a new company-wide advertising campaign. After having witnessed an American clothing company launch a UK marketing campaign for their new line of trousers by advertising them as 'pants' (which means 'underwear' in the UK), the company's directors want to avoid such embarassment by ensuring that their new advertising campaign is using language that is well-suited to the markets where the products will be sold. As an NLP data scientist, they have asked me to analyze data from the different regions in which the company operates and craft lists of words that most effectively convey positive sentiments (as well as a list of words to avoid, containing a list of words associated with negative sentiments). \n",
    "\n",
    "# The Data\n",
    "\n",
    "To accomplish my goal, I have a large amount of data from the review website 'Yelp'. There are two files in the dataset that will be of interest to me: a file listing information about the businesses being rated on the website, and a file containing the ratings themselves. The datasets have a field called 'business_id' that acts as a primary key. Each review in the dataset is accompanied by a star rating on a scale of 1-5, indicating the review writer's level of satisfaction. The dataset is not complete, so I will need to go through a step of exploring the data to better understand what I have to work with. \n",
    "\n",
    "# Preliminary Plan\n",
    "\n",
    "Based on my knowledge of the English-speaking world (I'm from the US and have traveled both there and the UK a great deal), my initial intuition is that we are likely to encounter 7-8 different dialects in the data for our target markets. If I'm correct, these would be: \n",
    "\n",
    "* **Southern American** (encompassing all states South of Virginia/West Virginia/Kentucky, stretching west toward Texas/Oklahoma)\n",
    "* **Midwestern American/Canadian** (Ohio, Michigan, Wisconsin, Minnesota, Illinois, plus the plains states like Iowa, Kansas, etc., Northward into parts of Manitoba/Saskatchewan) \n",
    "* **Northeastern American** (on the Eastern seaboard to the North of Maryland up to Maine, ending somewhere in Western Pennsylvania/New York State, at which point the dialect is more midwestern)\n",
    "* **Western American/Canadian** (all states along the Rocky Mountains, from Arizona/NM up to Alberta/British Columbia, including California)\n",
    "* **Eastern Canadian** (Canadian Maritime provinces plus Ontario and English-speaking areas of Quebec)\n",
    "* **Scottish** (There can be significant variation within this group, but generally any part of the UK north of Berwick-Upon-Tweed)\n",
    "* **Southern British English** (London and surrounding areas, Northward into the Midlands, West toward Cornwall and to the border with Wales)\n",
    "* **Northern British English** (represented by cities like Birmingham, Newcastle, Manchester, York, etc.)\n",
    "\n",
    "There are of course other smaller dialect areas (Louisiana Creole, Northern Irish, Welsh, etc), but for the sake of simplicity, let's start with these groups and see what we can do with our available data. \n",
    "\n",
    "# First Steps\n",
    "\n",
    "To begin with, I'll need to get an idea of what data we have available. It's all been loaded into MongoDB since the files are too big to work with on their own, so I'll begin by finding out approximately how many samples we have per geographic area. To do this, I'll query the businesses collection to find out how many businesses there are by state/city. The presence/absence of certain areas will determine my strategy going forward. As I'm not currently sure of how many businesses I have to work with, I'll run an initial query for 1 million businesses. I'll then put the data into a dictionary organized by state and city with a count of businesses by city, which can be used to examine how many businesses I have to work with in each of my dialect areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': {'Montreal': 1},\n",
      " 'AK': {'Chandler': 1},\n",
      " 'AL': {'La Paz': 1},\n",
      " 'AZ': {'': 1,\n",
      "        'Ahwahtukee': 1,\n",
      "        'Ahwatukee': 12,\n",
      "        'Ahwatukee Foothills Village': 1,\n",
      "        'Anthem': 135,\n",
      "        'Apache Junction': 186,\n",
      "        'Arlington': 2,\n",
      "        'Avondale': 427,\n",
      "        'Black Canyon City': 3,\n",
      "        'Buckeye': 163,\n",
      "        'Carefree': 54,\n",
      "        'Casa Grande': 201,\n",
      "        'Cave Creek': 266,\n",
      "        'Central': 1,\n",
      "        'Central City': 1,\n",
      "        'Central City Village': 3,\n",
      "        'Chandler': 2701,\n",
      "        'Chandler-Gilbert': 1,\n",
      "        'Coolidge': 21,\n",
      "        'Desert Ridge': 1,\n",
      "        'El Mirage': 63,\n",
      "        'Estrella Village': 1,\n",
      "        'Florence': 46,\n",
      "        'Fort McDowell': 8,\n",
      "        'Fort Mcdowell': 3,\n",
      "        'Fountain Hills': 204,\n",
      "        'Gelndale': 1,\n",
      "        'Gila Bend': 21,\n",
      "        'Gilbert': 1940,\n",
      "        'Gilbert, AZ': 1,\n",
      "        'Glbert': 1,\n",
      "        'Glendale': 2048,\n",
      "        'Glendale Az': 1,\n",
      "        'Gold Canyon': 42,\n",
      "        'Goldfield': 1,\n",
      "        'Goodyear': 507,\n",
      "        'Grand Canyon': 1,\n",
      "        'Guadalupe': 15,\n",
      "        'Higley': 17,\n",
      "        'Laveen': 96,\n",
      "        'Laveen Village': 8,\n",
      "        'Litchfield': 1,\n",
      "        'Litchfield Park': 120,\n",
      "        'Litchfield Park ': 1,\n",
      "        'Litchfield park': 1,\n",
      "        'MESA': 1,\n",
      "        'MMRP': 1,\n",
      "        'Maricopa': 165,\n",
      "        'Maricopa AZ': 1,\n",
      "        'Mesa': 3637,\n",
      "        'Mesa ': 1,\n",
      "        'Mesa AZ': 1,\n",
      "        'Metro Phoenix': 1,\n",
      "        'Morristown': 5,\n",
      "        'New River': 18,\n",
      "        'North Scottsdale': 1,\n",
      "        'Old Scottsdale': 1,\n",
      "        'PEORIA': 1,\n",
      "        'Paradise Valley': 90,\n",
      "        'Payson': 1,\n",
      "        'Peopria': 1,\n",
      "        'Peoria': 1081,\n",
      "        'Peoria ': 1,\n",
      "        \"Peoria Ave Citizen's Group\": 1,\n",
      "        'Pheonix': 2,\n",
      "        'Phoe': 1,\n",
      "        'Phoenix': 11851,\n",
      "        'Phoenix ': 1,\n",
      "        'Phoenix AZ': 1,\n",
      "        'Phoenix Sky Harbor Center': 1,\n",
      "        'Phoenix, AZ': 1,\n",
      "        'Phoneix': 2,\n",
      "        'Queen Creek': 383,\n",
      "        'Queen Valley': 1,\n",
      "        'Red Mountain': 1,\n",
      "        'Rio Verde': 8,\n",
      "        'Roosevelt': 2,\n",
      "        'SCOTTSDALE': 1,\n",
      "        'San Tan': 1,\n",
      "        'San Tan Valley': 100,\n",
      "        'Scotesdale': 1,\n",
      "        'Scottdale': 5,\n",
      "        'Scottsdale': 5638,\n",
      "        'Scottsdale AZ': 1,\n",
      "        'Sedona': 1,\n",
      "        'South Mountain': 1,\n",
      "        'Stanfield': 1,\n",
      "        'Sun City': 126,\n",
      "        'Sun City Anthem': 1,\n",
      "        'Sun City Grand': 1,\n",
      "        'Sun City West': 45,\n",
      "        'Sun Lakes': 16,\n",
      "        'Sunnyslope': 1,\n",
      "        'Surprise': 681,\n",
      "        'Surprise Crossing': 1,\n",
      "        'Tempe': 3043,\n",
      "        'Tempe ': 1,\n",
      "        'Tolleson': 90,\n",
      "        'Tonopah': 13,\n",
      "        'Tonto Basin': 7,\n",
      "        'Tortilla Flat': 3,\n",
      "        'Waddell': 8,\n",
      "        'Westworld Scottsdale': 1,\n",
      "        'Wickenburg': 78,\n",
      "        'Wittmann': 1,\n",
      "        'Youngtown': 30,\n",
      "        'glendale': 3,\n",
      "        'phoenix': 4,\n",
      "        'scottsdale': 3},\n",
      " 'BW': {'Bietigheim': 4,\n",
      "        'Bruchsal': 1,\n",
      "        'Durmersheim': 11,\n",
      "        'Eggenstein-Leopoldshafen': 13,\n",
      "        'Ettlingen': 56,\n",
      "        'Forchheim': 1,\n",
      "        'Karlsbad': 7,\n",
      "        'Karlsruhe': 906,\n",
      "        'Pfinztal': 11,\n",
      "        'Rheinstetten': 13,\n",
      "        'Rheinstetten-Mörsch': 1,\n",
      "        'Stutensee': 7,\n",
      "        'Stutensee-Blankenloch': 1,\n",
      "        'Waldbronn': 9,\n",
      "        'Weingarten': 11,\n",
      "        'Weingarten (Baden)': 3},\n",
      " 'CA': {'LA Puente': 1, 'Los Angeles': 1, 'Mesa': 1, 'San Diego': 1},\n",
      " 'EDH': {'': 1,\n",
      "         'Bruntsfield': 2,\n",
      "         'City of Edinburgh': 3,\n",
      "         'Cramond': 1,\n",
      "         'Cramond Bridge': 1,\n",
      "         'Currie': 1,\n",
      "         'Edimbourg': 1,\n",
      "         'Edinburgh': 3224,\n",
      "         'Edinburgh City of': 1,\n",
      "         'Firth of Forth': 3,\n",
      "         'Fort Kinnaird': 1,\n",
      "         'Fountainbridge': 1,\n",
      "         'Glasgow': 1,\n",
      "         'Ingliston': 1,\n",
      "         'Juniper Green': 5,\n",
      "         'Leith': 6,\n",
      "         'Loanhead': 1,\n",
      "         'London': 1,\n",
      "         'Midlothian': 1,\n",
      "         'New Town': 4,\n",
      "         'Newbridge': 3,\n",
      "         'Newington': 2,\n",
      "         'Old Town': 6,\n",
      "         'Portobello': 3,\n",
      "         'Queensferry': 3,\n",
      "         'Ratho': 1,\n",
      "         'Scotland': 1,\n",
      "         'South Gyle': 1,\n",
      "         'South Queensferry': 13,\n",
      "         'Stockbridge': 3,\n",
      "         'Water of Leith': 1},\n",
      " 'ELN': {'Musselburgh': 11},\n",
      " 'FIF': {'Inverkeithing': 3, 'North Queensferry': 2},\n",
      " 'FL': {'Gilbert': 1, 'Pittsburgh': 1},\n",
      " 'HAM': {'Stockbridge': 1},\n",
      " 'IL': {'Arlington Heights': 1,\n",
      "        'Burrito King': 1,\n",
      "        'Champaign': 505,\n",
      "        'Savoy': 15,\n",
      "        'Urbana': 286},\n",
      " 'KHL': {'Edinburgh': 1},\n",
      " 'MLN': {'Balerno': 1,\n",
      "         'Bonnyrigg': 1,\n",
      "         'Dalkeith': 9,\n",
      "         'Edinburgh': 132,\n",
      "         'Lasswade': 3,\n",
      "         'Loanhead': 5,\n",
      "         'Midlothian': 3,\n",
      "         'Musselburgh': 2,\n",
      "         'Penicuik': 2,\n",
      "         'Roslin': 1,\n",
      "         'Straiton': 2},\n",
      " 'MN': {'Eagan': 1},\n",
      " 'NC': {'Asheville': 1,\n",
      "        'Belmont': 83,\n",
      "        'Charlote': 1,\n",
      "        'Charlotte': 5694,\n",
      "        'Charlotte ': 2,\n",
      "        'Concord': 185,\n",
      "        'Concord Mills': 3,\n",
      "        'Fort Mill': 1,\n",
      "        'Gaston': 1,\n",
      "        'Harrisburg': 39,\n",
      "        'Huntersville': 39,\n",
      "        'Indian Trail': 67,\n",
      "        'Locust': 1,\n",
      "        'Mathews': 1,\n",
      "        'Mattews': 1,\n",
      "        'Matthews': 399,\n",
      "        'McAdenville': 1,\n",
      "        'Mint Hill': 50,\n",
      "        'Monroe': 2,\n",
      "        'Montgomery': 1,\n",
      "        'Mount Holly': 23,\n",
      "        'North Carolina': 1,\n",
      "        'Pineville': 210,\n",
      "        'Stallings': 11,\n",
      "        'University': 1,\n",
      "        'Waxhaw': 5,\n",
      "        'Weddington': 4,\n",
      "        'Wesley Chapel': 8},\n",
      " 'NM': {'Las Vegas': 1},\n",
      " 'NTH': {'Edinburgh': 1},\n",
      " 'NV': {'110 Las Vegas': 1,\n",
      "        'Blue Diamond': 1,\n",
      "        'Boulder City': 42,\n",
      "        'C Las Vegas': 1,\n",
      "        'Centennial Hills': 1,\n",
      "        'Central Henderson': 1,\n",
      "        'City Center': 1,\n",
      "        'Clark': 3,\n",
      "        'Enterprise': 6,\n",
      "        'Green Valley': 1,\n",
      "        'Henderson': 3145,\n",
      "        'Henderson NV': 1,\n",
      "        'Henderson and Las vegas': 1,\n",
      "        'Henderston': 1,\n",
      "        'Hendserson': 1,\n",
      "        'LAS  VEGAS': 1,\n",
      "        'Lake Las Vegas': 1,\n",
      "        'Las  Vegas': 1,\n",
      "        'Las Vegas': 19326,\n",
      "        'Las Vegas ': 6,\n",
      "        'Las Vegas East': 1,\n",
      "        'Las Vegas Strip': 1,\n",
      "        'Las Vegas, NV': 1,\n",
      "        'Las vegas': 2,\n",
      "        'LasVegas': 1,\n",
      "        'N E Las Vegas': 1,\n",
      "        'N Las Vegas': 19,\n",
      "        'N W Las Vegas': 1,\n",
      "        'N. Las Vegas': 2,\n",
      "        'NELLIS AFB': 1,\n",
      "        'Nboulder City': 1,\n",
      "        'Nellis': 1,\n",
      "        'Nellis AFB': 15,\n",
      "        'Nellis Afb': 3,\n",
      "        'Nellis Air Force Base': 1,\n",
      "        'North Las Vegas': 952,\n",
      "        'North Las Vegas ': 1,\n",
      "        'Paradise': 26,\n",
      "        'South Las Vegas': 3,\n",
      "        'Spring Valley': 6,\n",
      "        'Summerlin': 2,\n",
      "        'Summerlin South': 2,\n",
      "        'Sunrise': 1,\n",
      "        'W Henderson': 1,\n",
      "        'W Spring Valley': 1,\n",
      "        'W Summerlin': 1,\n",
      "        'West Las Vegas': 1,\n",
      "        'Whitney': 1},\n",
      " 'NW': {'Bocholt': 1},\n",
      " 'ON': {'Breslau': 1,\n",
      "        'Cambridge': 10,\n",
      "        'Conestogo': 2,\n",
      "        'Heidelberg': 1,\n",
      "        'Kitchener': 209,\n",
      "        'New Dundee': 1,\n",
      "        'St Jacobs': 4,\n",
      "        'St. Jacobs': 4,\n",
      "        'Victoria Park': 1,\n",
      "        'Waterloo': 297},\n",
      " 'PA': {'Allegheny': 1,\n",
      "        'Allentown': 2,\n",
      "        'Aspinwall': 12,\n",
      "        'Avalon': 4,\n",
      "        'Banksville': 1,\n",
      "        'Bellevue': 24,\n",
      "        'Bellvue': 3,\n",
      "        'Bethel Park': 3,\n",
      "        'Blawnox': 2,\n",
      "        'Bloomfield': 1,\n",
      "        'Braddock': 4,\n",
      "        'Brentwood': 9,\n",
      "        'Bridgeville': 11,\n",
      "        'Brookline': 2,\n",
      "        'Carnegie': 62,\n",
      "        'Castle Shannon': 6,\n",
      "        'Chateau': 1,\n",
      "        'Crafton': 7,\n",
      "        'Dormont': 4,\n",
      "        'Downtown': 3,\n",
      "        'Dravosburg': 4,\n",
      "        'East Liberty': 2,\n",
      "        'East McKeesport': 1,\n",
      "        'Edgewood': 2,\n",
      "        'Etna': 1,\n",
      "        'Green Tree': 1,\n",
      "        'Heidelberg': 6,\n",
      "        'Homestead': 98,\n",
      "        'Ingram': 1,\n",
      "        'Lawrenceville': 4,\n",
      "        'Lower Lawrenceville': 1,\n",
      "        'Mc Kees Rocks': 12,\n",
      "        'McKees Rocks': 17,\n",
      "        'McKeesport': 2,\n",
      "        'Mckees Rocks': 5,\n",
      "        'Millvale': 6,\n",
      "        'Mount Lebanon': 13,\n",
      "        'Mount Washington': 2,\n",
      "        'Mt Lebanon': 4,\n",
      "        'Mt. Lebanon': 2,\n",
      "        'Mt. Oliver Boro': 1,\n",
      "        'Munhall': 19,\n",
      "        'Oakland': 3,\n",
      "        'PITTSBURGH': 1,\n",
      "        'Pittsburg': 1,\n",
      "        'Pittsburgh': 3627,\n",
      "        'Rankin': 2,\n",
      "        'Regent Square': 1,\n",
      "        'Robinson Township': 1,\n",
      "        'Shady Side': 1,\n",
      "        'Shadyside': 3,\n",
      "        'Sharpsburg': 4,\n",
      "        'Side Slopes': 1,\n",
      "        'South Hills': 1,\n",
      "        'Southside Flats': 1,\n",
      "        'Squirrel Hill': 3,\n",
      "        'Stowe Township': 1,\n",
      "        'Swissvale': 4,\n",
      "        'Upper Saint Clair': 1,\n",
      "        'Verona': 5,\n",
      "        'West Homestead': 12,\n",
      "        'West Mifflin': 36,\n",
      "        'Whitehall': 1,\n",
      "        'Wilkinsburg': 9,\n",
      "        'pleasant hills': 1},\n",
      " 'QC': {'Anjou': 11,\n",
      "        \"Baie-D'urfe\": 1,\n",
      "        'Beaconsfield': 5,\n",
      "        'Blainville': 2,\n",
      "        'Bois-Des-Filion': 1,\n",
      "        'Boisbriand': 7,\n",
      "        'Brossard': 72,\n",
      "        'Charlemagne': 1,\n",
      "        'Chomedey, Laval': 1,\n",
      "        'Communauté-Urbaine-de-Montréal': 2,\n",
      "        'Cote Saint-Luc': 2,\n",
      "        'Cote-Saint-Luc': 2,\n",
      "        'Deux-Montagnes': 1,\n",
      "        'Deux-Montagnes Regional County Municipality': 1,\n",
      "        'Dollard-Des Ormeaux': 2,\n",
      "        'Dollard-Des-Ormeaux': 30,\n",
      "        'Dollard-des-Ormeaux': 50,\n",
      "        'Dorval': 43,\n",
      "        'Fabreville': 1,\n",
      "        'Greenfield Park': 8,\n",
      "        'Kahnawake': 1,\n",
      "        'Kirkland': 26,\n",
      "        \"L'Île-Bizard\": 2,\n",
      "        'La Prairie': 10,\n",
      "        'La Salle': 4,\n",
      "        'LaSalle': 5,\n",
      "        'Lachenaie': 1,\n",
      "        'Lachine': 29,\n",
      "        'Lasalle': 49,\n",
      "        'Laval': 222,\n",
      "        'Le Sud-Ouest': 1,\n",
      "        'Longueuil': 32,\n",
      "        'Mascouche': 1,\n",
      "        'Mirabel': 1,\n",
      "        'Mont-Royal': 14,\n",
      "        'Montral': 1,\n",
      "        'Montreal': 95,\n",
      "        'Montreal-Est': 1,\n",
      "        'Montreal-Nord': 3,\n",
      "        'Montreal-Ouest': 2,\n",
      "        'Montreal-West': 2,\n",
      "        'Montréal': 4371,\n",
      "        'Montréal-Nord': 2,\n",
      "        'Montréal-Ouest': 1,\n",
      "        'Montéal': 1,\n",
      "        'Mount Royal': 1,\n",
      "        'Outremont': 35,\n",
      "        'Pierrefonds': 26,\n",
      "        'Point Claire': 1,\n",
      "        'Pointe Claire': 2,\n",
      "        'Pointe-Aux-Trembles': 4,\n",
      "        'Pointe-Claire': 52,\n",
      "        'Québec': 1,\n",
      "        'Repentigny': 1,\n",
      "        'Rosemere': 7,\n",
      "        'Rosemère': 5,\n",
      "        'Roxboro': 2,\n",
      "        'Saint Laurent': 4,\n",
      "        'Saint Leonard': 1,\n",
      "        'Saint-Eustache': 8,\n",
      "        'Saint-Henri': 1,\n",
      "        'Saint-Hubert': 3,\n",
      "        'Saint-Lambert': 11,\n",
      "        'Saint-Laurent': 75,\n",
      "        'Saint-Leonard': 25,\n",
      "        'Saint-Léonard': 1,\n",
      "        'Saint-laurent': 2,\n",
      "        'Sainte-Anne-De-Bellevue': 8,\n",
      "        'Sainte-Anne-de-Bellevue': 4,\n",
      "        'Sainte-Anne-des-Plaines': 1,\n",
      "        'Sainte-Genevieve': 1,\n",
      "        'Sainte-Therese': 1,\n",
      "        'Sainte-Thérèse': 3,\n",
      "        'St Leonard': 1,\n",
      "        'St-Laurent': 1,\n",
      "        'St-Leonard': 2,\n",
      "        'Ste-Rose': 1,\n",
      "        'Terrebonne': 7,\n",
      "        'Verdun': 109,\n",
      "        'Vimont': 4,\n",
      "        'Westmount': 59,\n",
      "        'Île des Soeurs': 1},\n",
      " 'RP': {'Hagenbach': 5,\n",
      "        'Jockgrim': 2,\n",
      "        'Neuburg am Rhein': 2,\n",
      "        'Wörth': 1,\n",
      "        'Wörth am Rhein': 8},\n",
      " 'SC': {'Charlotte': 1,\n",
      "        'Clover': 4,\n",
      "        'Fort  Mill': 1,\n",
      "        'Fort Mill': 286,\n",
      "        'Ft. Mill': 2,\n",
      "        'Indian Land': 8,\n",
      "        'Lake Wylie': 7,\n",
      "        'Rock Hill': 3,\n",
      "        'Tega Cay': 13},\n",
      " 'SCB': {'Bonnyrigg': 1, 'Edinburgh': 1},\n",
      " 'TAM': {'Las Vegas': 1},\n",
      " 'TX': {'Dallas': 1, 'Garland': 1, 'Phoenix': 1},\n",
      " 'WI': {'Columbus': 2,\n",
      "        'Cottage Grove': 20,\n",
      "        'Dane': 2,\n",
      "        'De Forest': 21,\n",
      "        'DeForest': 10,\n",
      "        'Deforest': 2,\n",
      "        'Fitchburg': 136,\n",
      "        'Fitchburgh': 1,\n",
      "        'MADISON': 1,\n",
      "        'Madison': 2278,\n",
      "        'Mc Farland': 17,\n",
      "        'McFarland': 16,\n",
      "        'Mcfarland': 2,\n",
      "        'Middleton': 222,\n",
      "        'Monona': 85,\n",
      "        'Oregon': 4,\n",
      "        'Shorewood Hills': 1,\n",
      "        'Stoughton': 3,\n",
      "        'Sun Prairie': 129,\n",
      "        'Sun PrairieÊ': 1,\n",
      "        'Verona': 62,\n",
      "        'Waunakee': 41,\n",
      "        'Windsor': 10},\n",
      " 'XGL': {'Edinburgh': 1}}\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "from collections import Counter\n",
    "\n",
    "client = MongoClient()\n",
    "db = client.newYorkerTest\n",
    "businesses = db.businesses\n",
    "\n",
    "cursor = businesses.find().limit(1000000)\n",
    "city_state_list = []\n",
    "for doc in cursor:\n",
    "    city_state_list.append([doc['city'], doc['state']])\n",
    "    \n",
    "count_dict = {}\n",
    "\n",
    "for row in city_state_list:\n",
    "    if not row[1] in count_dict:\n",
    "        count_dict[row[1]] = {row[0]:1}\n",
    "    else:\n",
    "        if row[0] in count_dict[row[1]]:\n",
    "            count_dict[row[1]][row[0]] += 1\n",
    "        else:\n",
    "            count_dict[row[1]][row[0]] = 1\n",
    "    \n",
    "\n",
    "pprint.pprint(count_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so the data looks like it will give us a bit more of a limited set of options than would be ideal. We have a large amount of data from Arizona and Nevada, which should provide good data for the Western American/Canadian dialect. For the Southern American dialect, there is only a significant amount of data for North/South Carolina, and most of it appears to be from the Charlotte area. For the midwest, there appears to be a big grouping of data around Urbana/Champaign Illinois, as well as a good set of data in Madison Wisconsin (notably, all of the centers so far seem to be major university towns). The Northeast American dialect seems a bit under-represented here; the only state on this list that could fall into the Northeast American dialect area is Pennsylvania, and the only city for which there is significant data is Pittsburgh (whose location in the far Western part of Pennsylvania makes it almost midwestern from a cultural/dialectical standpoint). In Canada, we have a fair amount of samples from Quebec and Ontario, which should cover our Eastern Canadian dialect area. In the UK, our data all seems to be around Scotland, particularly Edinburgh. \n",
    "\n",
    "I can therefore use this data to provide recommendations about the wording to be used in campaigns targeting the American/Canadian midwest, west, and south, along with Eastern Canada and Scotland. We don't have sufficient data to provide recommendations for non-Scottish UK English or for American Northeastern English. \n",
    "\n",
    "Now, on to the strategy. \n",
    "\n",
    "# Strategy\n",
    "\n",
    "I'd like to start by building queries that will pull a sufficient number of reviews from each location. Now, this model will serve more as a proof of concept than as a production model, so the focus should be on rapid prototyping rather than on speed/memory optimization. The reviews dataset looks to have a total of about 2,5 million reviews; some of the reviews will not be of interest to me here (those in German and French), and for a proof-of-concept model, samples of 10.000 per dialect group should be sufficient. If the model works and my group decides to implement it, incorporating some map/reduce steps and distributing this dataset over a cluster would make it possible to do a more thorough analysis with all of the data available.\n",
    "\n",
    "Since I'm being asked to give a list of words with positive associations and negative associations, a classifier model might be a good way to go about solving the problem. If I train an accurate classifier model on each dataset, I can use the list of most informative features as my list of words to use/avoid when crafting a marketing campaign for each market. \n",
    "\n",
    "To prepare the data for my classifier, I'll need to process it to remove punctuation and lemmatize (break down to its most basic form) all of the words in each review. For each dialect, I'll build a set of features (reviews that have been processed for punctuation and lemmatization) and labels (the sentiment associated with the feature sets). Since each review comes with a star rating indicating the reviewer's level of satisfaction, we can use that as a marker for if a review is positive or negative. The end goal is a binary positive/negative sentiment label, so I'll exclude 3-star reviews (since the sentiment of a three-star review is a bit nebulous). \n",
    "\n",
    "To start off, I'm going to build five lists of business IDs associated with businesses in each dialect area. Once those lists are built, I'm going to create two classes: one for individual review objects, and one for a feature/label set of reviews. I'll then train/test a few algorithms until I find one with an acceptable level of accuracy. With a decent algorithm in place, it'll be a simple matter of getting the most informative positive/negative features for each group, then I can present the findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "south:  7160\n",
      "midwest:  3874\n",
      "canada:  6121\n",
      "west:  60091\n",
      "scotland:  3466\n"
     ]
    }
   ],
   "source": [
    "business_list_midwest = []\n",
    "business_list_south = []\n",
    "business_list_west = []\n",
    "business_list_canada = []\n",
    "business_list_scotland = []\n",
    "\n",
    "cursor = businesses.find().limit(1000000)\n",
    "for doc in cursor:\n",
    "    if doc['state'] == 'WI':\n",
    "        business_list_midwest.append(doc['business_id'])\n",
    "    elif doc['state'] == 'IL':\n",
    "        business_list_midwest.append(doc['business_id'])\n",
    "    elif doc['state'] == 'AZ':\n",
    "        business_list_west.append(doc['business_id'])\n",
    "    elif doc['state'] == 'NV':\n",
    "        business_list_west.append(doc['business_id'])\n",
    "    elif doc['state'] == 'NC':\n",
    "        business_list_south.append(doc['business_id'])\n",
    "    elif doc['state'] == 'SC':\n",
    "        business_list_south.append(doc['business_id'])\n",
    "    elif doc['state'] == 'ON':\n",
    "        business_list_canada.append(doc['business_id'])\n",
    "    elif doc['state'] == 'QC':\n",
    "        business_list_canada.append(doc['business_id'])\n",
    "    elif doc['state'] == 'IL':\n",
    "        business_list_midwest.append(doc['business_id'])\n",
    "    elif doc['state'] in ['EDH', 'FIF', 'MLN', 'KHL', 'NTH', 'XGL']:\n",
    "        business_list_scotland.append(doc['business_id'])\n",
    "        \n",
    "print('south: ', len(business_list_south))\n",
    "print('midwest: ', len(business_list_midwest))\n",
    "print('canada: ', len(business_list_canada))\n",
    "print('west: ', len(business_list_west))\n",
    "print('scotland: ', len(business_list_scotland))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like I have at least 3000 businesses from each area to examine; this should be sufficient to build a classifier, considering that each business has a decent number of reviews. \n",
    "\n",
    "For the next step, I'll use the business IDs I found before to pull 10.000 reviews (1, 2, 4, or 5-star only) for each location. to save memory, I'll dump each feature/label dictionary into a pickle after each region, therefore keeping me from having to hold 50.000 reviews in memory. \n",
    "\n",
    "first, though, I should build my classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from spacy.en import English\n",
    "from langdetect import detect\n",
    "\n",
    "parser = English()\n",
    "\n",
    "class review():\n",
    "    \n",
    "    def __init__(self, review_doc):\n",
    "        self.text = self.process_text(review_doc['text'])\n",
    "        self.parsed = self.parse(self.text)\n",
    "        self.lemmatized = self.lemmatize(self.parsed)\n",
    "        self.stars = review_doc['stars']\n",
    "\n",
    "    def process_text(self, string_input):\n",
    "        table = str.maketrans({key: None for key in string.punctuation})\n",
    "        return string_input.translate(table).lower()\n",
    "\n",
    "    def parse(self, text):\n",
    "        try:\n",
    "            return parser(text)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def lemmatize(self, parsed):\n",
    "        try:\n",
    "            output = []\n",
    "            for token in parsed:\n",
    "                output.append(token.lemma_)\n",
    "            return ' '.join(i for i in output)\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "class reviewSet():\n",
    "\n",
    "    def __init__(self, business_list, canada = False):\n",
    "        self.client = MongoClient()\n",
    "        self.db = self.client.newYorkerTest\n",
    "        self.reviews = self.db.reviews\n",
    "        self.raw = self.pull_reviews(business_list, canada)\n",
    "        self.features, self.labels = self.separate(self.raw)\n",
    "        self.dict = {'features':self.features, 'labels':self.labels}\n",
    "\n",
    "    def pull_reviews(self, business_list, canada):\n",
    "        outList = []\n",
    "        query = {'business_id':{'$in':business_list},'stars':{'$ne':3}}\n",
    "        for i in self.reviews.find(query):\n",
    "            if len(outList) <= 10000:\n",
    "            #Note: here, I'm adding a piece of code for language detection, as I suspsect a good number of reviews from \n",
    "            #Quebec will be in French. Since SpaCy only has support for English (and since my assignment is for building an English-language ad campaign), \n",
    "            #we're going to skip over non-English reviews for now. If I were asked to run this exercise for a French- (or German)-language ad campaign, \n",
    "            #the slower NLTK package does have support for languages other than English, so we could use it instead. \n",
    "                try:\n",
    "                    if canada:\n",
    "                        if detect(i['text']) == 'en': \n",
    "                            outList.append(review(i))\n",
    "                    else:\n",
    "                        outList.append(review(i))\n",
    "                except:\n",
    "                    continue\n",
    "        return outList\n",
    "\n",
    "    def separate(self, raw):\n",
    "        features = []\n",
    "        labels = []\n",
    "        for i in raw: \n",
    "            features.append(i.lemmatized)\n",
    "            if i.stars >=4:\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "        return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the classes written, we can move on to deciding on an algorithm. After we have our first set of data, we can divide it into train/test sets, vectorize the data, and build a function to decide on how effective our classifier is.\n",
    "\n",
    "For the vectorizer, we'll go with TF-IDF, since some reviews are much longer than others. Note that this step takes a little while to process, probably about 5 minutes total. This is normal, as the lemmatization step uses a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "midwest_reviews = reviewSet(business_list_midwest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, let's do a train/test run with a couple of classifiers and see which ones deliver the best results. Since I'm looking to predict a binary value (positive/negative), this is a classification problem rather than a regression problem. I'm going to try three classifiers: logistic regression (maximum entropy), support vector machines, and stochastic gradient descent classifiers. While an ensemble classifier might be more accurate, they don't allow one to see the most informative features in text analysis, so they won't be useful to my task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "--------------------------------------------------------------\n",
      "Accuracy:  0.886669995007\n",
      "Precision: 0.972437197709\n",
      "F1 Score:  0.92180502928\n",
      "Recall:    0.874509803922\n",
      "    \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "--------------------------------------------------------------\n",
      "Accuracy:  0.91412880679\n",
      "Precision: 0.967128338035\n",
      "F1 Score:  0.938571428571\n",
      "Recall:    0.920812894184\n",
      "    \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "--------------------------------------------------------------\n",
      "Accuracy:  0.904643035447\n",
      "Precision: 0.959990995515\n",
      "F1 Score:  0.931369026231\n",
      "Recall:    0.91914893617\n",
      "    \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=10, n_jobs=1,\n",
      "       penalty='L1', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "--------------------------------------------------------------\n",
      "Accuracy:  0.89765351972\n",
      "Precision: 0.964620091279\n",
      "F1 Score:  0.927587424938\n",
      "Recall:    0.900548696845\n",
      "    \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=10, n_jobs=1,\n",
      "       penalty='L2', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "--------------------------------------------------------------\n",
      "Accuracy:  0.912131802297\n",
      "Precision: 0.966518777916\n",
      "F1 Score:  0.937187723055\n",
      "Recall:    0.918824352694\n",
      "    \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=10, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "--------------------------------------------------------------\n",
      "Accuracy:  0.910134797803\n",
      "Precision: 0.964948182944\n",
      "F1 Score:  0.935668334525\n",
      "Recall:    0.918596491228\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "def test_classifier_effectiveness(clf, data, test_pct):\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "    threshhold = int(len(data.dict['features'])*test_pct)\n",
    "    x_train = vectorizer.fit_transform(data.dict['features'][threshhold:])\n",
    "    x_test = vectorizer.transform(data.dict['features'][:threshhold])\n",
    "    y_train = data.dict['labels'][threshhold:]\n",
    "    y_test = data.dict['labels'][:threshhold]\n",
    "    clf.fit(x_train, y_train)\n",
    "    predictions = clf.predict(x_test)\n",
    "    print(clf)\n",
    "    print('--------------------------------------------------------------')\n",
    "    print('Accuracy:  ' + str(metrics.accuracy_score(predictions, y_test)))\n",
    "    print('Precision: ' + str(metrics.average_precision_score(predictions, y_test)))\n",
    "    print('F1 Score:  ' + str(metrics.f1_score(predictions, y_test))) \n",
    "    print('Recall:    ' + str(metrics.recall_score(predictions, y_test)))\n",
    "    print('    ')\n",
    "    return clf, vectorizer\n",
    "    \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "mdw_clf_1, mdw_vec_1 = test_classifier_effectiveness(LogisticRegression(solver = \"sag\"), midwest_reviews, .2)\n",
    "mdw_clf_2, mdw_vec_2 = test_classifier_effectiveness(LinearSVC(loss = 'squared_hinge', penalty = 'l2', dual = False), midwest_reviews, .2)\n",
    "mdw_clf_3, mdw_vec_3 = test_classifier_effectiveness(LinearSVC(loss = 'squared_hinge', penalty = 'l1', dual = False), midwest_reviews, .2)\n",
    "mdw_clf_4, mdw_vec_4 = test_classifier_effectiveness(SGDClassifier(alpha = .0001, n_iter = 10, penalty = 'L1'), midwest_reviews, .2)\n",
    "mdw_clf_5, mdw_vec_5 = test_classifier_effectiveness(SGDClassifier(alpha = .0001, n_iter = 10, penalty = 'L2'), midwest_reviews, .2)\n",
    "mdw_clf_6, mdw_vec_6 = test_classifier_effectiveness(SGDClassifier(alpha = .0001, n_iter = 10, penalty = 'elasticnet'), midwest_reviews, .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it looks like the SVC classifier with the l2 penalty is giving the best results, let's re-fit the classifier on the whole midwest dataset, pickle the classifier and vectorizer, then move on to do the same for the other regional datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['midwest_classifier.pkl',\n",
       " 'midwest_classifier.pkl_01.npy',\n",
       " 'midwest_classifier.pkl_02.npy',\n",
       " 'midwest_classifier.pkl_03.npy']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "classifier = LinearSVC(loss = 'squared_hinge', penalty = 'l2', dual = False)\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(midwest_reviews.dict['features'])\n",
    "classifier.fit(X, midwest_reviews.dict['labels'])\n",
    "\n",
    "joblib.dump(vectorizer, 'midwest_vectorizer.pkl')\n",
    "joblib.dump(classifier, 'midwest_classifier.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a clear winner among our classifier models, we can move on to implementing our final analysis. The first stage is going to be generating a dataset for each geographic area. This is currently a very memory-intensive and slow task; generating a single dataset can take more than two hours due to the lemmatization steps. If this were to be a production task, I could realize some time savings by distributing the memory-intensive part (the lemmatization of the review texts) over a cluster of machines using Hadoop, but for this demonstration, I can live with the long processing time. \n",
    "\n",
    "**Attention**: if you are reviewing this and trying to reproduce my code, you might want to skip the following cell and go to the next one. Depending on your system, the next section could take 20-30 minutes to run, and I would not recommend it if you have less than 8 GB of RAM. If that's the case, skip to the next cell and re-load the pickled data, which will be much less memory-intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have pulled the data I need for each region, I'll add in a code block that allows a user here to reopen a dataset without re-loading everything from the database. The default will be to just re-load the classifier and vectorizer, which will allow one to view the most informative features (which is, after all, the point of this exercise), with the option of re-loading the whole dataset in case one wants to try a different classifier model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "def pull_classify_and_dump(dataset, name, canada = False):\n",
    "    reviews = reviewSet(dataset, canada)\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "    classifier = SGDClassifier(alpha = .0001, n_iter = 10, penalty = 'L2')\n",
    "    X = vectorizer.fit_transform(reviews.dict['features'])\n",
    "    classifier.fit(X, reviews.dict['labels'])\n",
    "    joblib.dump(vectorizer, name + '_vectorizer.pkl')\n",
    "    joblib.dump(classifier, name + '_classifier.pkl')\n",
    "    joblib.dump(dataset, name + '_dataset.pkl')\n",
    "    del reviews\n",
    "    \n",
    "pull_classify_and_dump(business_list_south, 'south')\n",
    "pull_classify_and_dump(business_list_west, 'west')\n",
    "pull_classify_and_dump(business_list_canada, 'canada', canada = True)\n",
    "pull_classify_and_dump(business_list_scotland, 'scotland')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reload(region):\n",
    "    vectorizer = joblib.load(region + '_vectorizer.pkl')\n",
    "    classifier = joblib.load(region + '_classifier.pkl')\n",
    "    return vectorizer, classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the ability to reload data in place, the final step is to accomplish my main goal: find the top/bottom n most positive words for a given locale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def n_most_least_positive(region, n, print_output = True):\n",
    "    vectorizer, classifier = reload(region)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    top_n = np.argsort(classifier.coef_[0])[-n:]\n",
    "    bottom_n = np.argsort(classifier.coef_[0])[:n]\n",
    "    positive_words = (feature_names[j] for j in top_n)\n",
    "    negative_words = (feature_names[j] for j in bottom_n)\n",
    "    if print_output:\n",
    "        print('Top ' + str(n) + ' most positively-associated words in region: ' + region)\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(list(positive_words))\n",
    "        print('            ')\n",
    "        print('Top ' + str(n) + ' most negatively-associated words in region: ' + region)\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(list(negative_words))\n",
    "    return positive_words, negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most positively-associated words in region: south\n",
      "------------------------------------------------------------------------\n",
      "['happy', 'bit', 'enjoy', 'fantastic', 'little', 'clean', 'easy', 'definitely', 'wonderful', 'nice', 'good', 'friendly', 'awesome', 'helpful', 'amazing', 'perfect', 'excellent', 'love', 'delicious', 'great']\n",
      "            \n",
      "Top 20 most negatively-associated words in region: south\n",
      "------------------------------------------------------------------------\n",
      "['disappointed', 'horrible', 'bland', 'poor', 'bad', 'terrible', 'rude', 'ok', 'lack', 'mediocre', 'awful', 'meh', 'cold', 'gross', 'dirty', 'tasteless', 'disappointing', 'charge', 'refund', 'leave']\n"
     ]
    }
   ],
   "source": [
    "positive_south, negative_south = n_most_least_positive('south', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most positively-associated words in region: scotland\n",
      "------------------------------------------------------------------------\n",
      "['brilliant', 'fun', 'pron', 'glad', 'fresh', 'relaxed', 'range', 'helpful', 'edinburgh', 'tasty', 'lunch', 'enjoy', 'definitely', 'perfect', 'excellent', 'love', 'friendly', 'amazing', 'great', 'delicious']\n",
      "            \n",
      "Top 20 most negatively-associated words in region: scotland\n",
      "------------------------------------------------------------------------\n",
      "['poor', 'disappointing', 'bland', 'awful', 'bad', 'disappointed', 'ok', 'rude', 'tasteless', 'meh', 'horrible', 'overpriced', 'sorry', 'waste', 'okay', 'mediocre', 'mall', 'refund', 'particularly', 'instead']\n"
     ]
    }
   ],
   "source": [
    "positive_scotland, negative_scotland = n_most_least_positive('scotland', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most positively-associated words in region: canada\n",
      "------------------------------------------------------------------------\n",
      "['attentive', 'wonderful', 'schwartz', 'spot', 'favorite', 'tasty', 'enjoy', 'awesome', 'definitely', 'montreal', 'bit', 'perfectly', 'fantastic', 'friendly', 'perfect', 'excellent', 'love', 'amazing', 'great', 'delicious']\n",
      "            \n",
      "Top 20 most negatively-associated words in region: canada\n",
      "------------------------------------------------------------------------\n",
      "['bad', 'bland', 'terrible', 'mediocre', 'disappointing', 'horrible', 'avoid', 'rude', 'disappointed', 'awful', 'overpriced', 'average', 'dirty', 'underwhelming', 'meh', 'overrated', 'pay', 'dry', 'disappointment', 'lack']\n"
     ]
    }
   ],
   "source": [
    "positive_canada, negative_canada = n_most_least_positive('canada', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most positively-associated words in region: midwest\n",
      "------------------------------------------------------------------------\n",
      "['tasty', 'professional', 'enjoy', 'solid', 'friendly', 'free', 'definitely', 'highly', 'favorite', 'helpful', 'reasonable', 'fantastic', 'perfect', 'excellent', 'good', 'awesome', 'love', 'amazing', 'great', 'delicious']\n",
      "            \n",
      "Top 20 most negatively-associated words in region: midwest\n",
      "------------------------------------------------------------------------\n",
      "['bad', 'bland', 'horrible', 'meh', 'overpriced', 'awful', 'terrible', 'ok', 'tell', 'unfortunately', 'gross', 'rude', 'poor', 'lack', 'subpar', 'money', 'disappointing', 'okay', 'disgust', 'mediocre']\n"
     ]
    }
   ],
   "source": [
    "positive_midwest, negative_midwest = n_most_least_positive('midwest', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, even here I can see some regional variations. For instance, the word 'brilliant' is a strongly-associated positive word in Scotland, but not in the US South, Midwest, or Candada; this makes sense to me, as 'brilliant' in the US more means 'highly intelligent', while I have known Scots to use the word 'brilliant' to mean 'wonderful'. The word 'fresh' appears on the list for Scotland, but not on the other regions, so perhaps the marketing folks should emphasize the freshness of our restaurant's food when making advertisements for the Scottish market. \n",
    "\n",
    "Some of the words look like they may have ended up with positive/negative associations by accident (I don't really know what 'pron' means, and 'schwartz' on the Canadian list is almost certainly a result of the language detector failing to catch some German-language reviews), so some more work is needed before this becomes a production model, but for a proof-of-concept, it should suffice. \n",
    "\n",
    "As a final step, let's set up a function that will allow someone to re-run this model for a different region when new data becomes available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "def top_n_by_region(inputData, name, n, states = True, cities = False, check_english = False):\n",
    "    client = MongoClient()\n",
    "    db = client.newYorkerTest\n",
    "    businesses = db.businesses\n",
    "    cursor = businesses.find().limit(1000000)\n",
    "    region_businesses = []\n",
    "    if states:\n",
    "        for element in cursor:\n",
    "            if states:\n",
    "                if type(inputData) is list:\n",
    "                    if element['state'] in inputData:\n",
    "                        region_businesses.append(element['business_id'])\n",
    "                elif type(inputData) is str:\n",
    "                    if element['state'] == inputData:\n",
    "                        region_businesses.append(element['business_id'])\n",
    "    if cities:\n",
    "        for element in cursor:\n",
    "            if type(inputData) is list:\n",
    "                if element['city'] in inputData:\n",
    "                    region_businesses.append(element['business_id'])\n",
    "            elif type(inputData) is str:\n",
    "                if element['city'] == inputData:\n",
    "                    region_businesses.append(element['business_id'])\n",
    "    pull_classify_and_dump(region_businesses, name, check_english)\n",
    "    n_most_least_positive(name, n, print_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to check that it works, let's run this operation over the data from Pittsburgh, which we excluded earlier due to it being hard to fit into one of our regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most positively-associated words in region: Pennsylvania\n",
      "------------------------------------------------------------------------\n",
      "['bit', 'complaint', 'walter', 'perfectly', 'fresh', 'wonderful', 'reasonable', 'enjoy', 'happy', 'favorite', 'fantastic', 'excellent', 'friendly', 'perfect', 'awesome', 'good', 'amazing', 'love', 'delicious', 'great']\n",
      "            \n",
      "Top 20 most negatively-associated words in region: Pennsylvania\n",
      "------------------------------------------------------------------------\n",
      "['terrible', 'bad', 'rude', 'disappointing', 'bland', 'mediocre', 'horrible', 'disappointed', 'ok', 'awful', 'meh', 'ridiculous', 'poor', 'tell', 'overpriced', 'okay', 'lack', 'poorly', 'stale', 'pay']\n"
     ]
    }
   ],
   "source": [
    "top_n_by_region('PA', 'Pennsylvania', 20, states = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it! Our model now works and a user can input a state, city, or list of states/cities, the number of words they want to see, and whether or not they think language detection will be necessary for the given region, and in return they will get a list of words that convey positive/negative feelings in the locale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
